{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Rightmove data\n",
    "\n",
    "> Aim: Query and explore the Rightmove data.\n",
    "\n",
    "## Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go to https://www.rightmove.co.uk/house-prices.html\n",
    "# enter your specified search\n",
    "# go to page 2\n",
    "# copy the URL up to *Number= and replace the URL below\n",
    "\n",
    "url = \"https://www.rightmove.co.uk/house-prices/w3-9jj.html?radius=0.5&soldIn=5&tenure=FREEHOLD&pageNumber=\"\n",
    "\n",
    "def get_sold_property_info_postcode(url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    n_pages = soup.find_all(\"div\", class_=\"dsrm_dropdown_section\")[0].find_all(\"span\")[1]\n",
    "    n_pages = int(n_pages.text.replace(\"of \", \"\"))\n",
    "\n",
    "    print(f\"There's {n_pages} of results.\")\n",
    "\n",
    "    time.sleep(3)\n",
    "\n",
    "    properties_info_pages = []\n",
    "\n",
    "    for i in range(n_pages):\n",
    "\n",
    "        print(f\"scraping page {i}\")\n",
    "        response = requests.get(url + str(i + 1) , headers=headers)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        properties_info_pages.append(get_sold_property_info(soup))\n",
    "        \n",
    "        time.sleep(3)\n",
    "    \n",
    "    properties_info_pages = pd.concat(properties_info_pages)\n",
    "\n",
    "    return properties_info_pages\n",
    "\n",
    "def get_sold_property_info(soup):\n",
    "    property_cards = soup.find_all(\"a\", attrs={\"data-testid\": \"propertyCard\"})\n",
    "\n",
    "    properties_info = []\n",
    "    \n",
    "    for property_card in property_cards:\n",
    "        address = property_card.find(\"h2\").text\n",
    "\n",
    "        property_type = property_card.find_all(\"div\", attrs={\"aria-label\": re.compile(r\"property type:\", re.IGNORECASE)})\n",
    "        if property_type:\n",
    "            property_type = property_type[0].text.replace(\"Property Type: \", \"\")\n",
    "        else:\n",
    "            property_type = \"\"\n",
    "\n",
    "        bedrooms = property_card.find_all(\"div\", attrs={\"aria-label\": re.compile(r\"bedrooms\", re.IGNORECASE)})\n",
    "        if bedrooms:\n",
    "            bedrooms = int(bedrooms[0].text.replace(\"Bedrooms: \", \"\"))\n",
    "        else:\n",
    "            bedrooms = pd.NA\n",
    "\n",
    "        dates, prices = extract_dates_prices(property_card)\n",
    "\n",
    "        property_info = pd.DataFrame({\n",
    "            \"property_type\": property_type,\n",
    "            \"address\": address,\n",
    "            \"date\": dates,\n",
    "            \"price\": prices,\n",
    "            \"bedrooms\": bedrooms,\n",
    "        })\n",
    "\n",
    "        properties_info.append(property_info)\n",
    "    \n",
    "    properties_info = pd.concat(properties_info).reset_index(drop=True)\n",
    "\n",
    "    return properties_info\n",
    "\n",
    "def extract_dates_prices(property_card):\n",
    "    prices_dates = property_card.find_all(\"td\")[2:]\n",
    "\n",
    "    dates = []\n",
    "    prices = []\n",
    "\n",
    "    for i, p in enumerate(prices_dates):\n",
    "        # Reached the end of available dates.\n",
    "        if p.text == \"\":\n",
    "            break\n",
    "\n",
    "        price = p.text.replace(\"\\x00\", \"\")\n",
    "        \n",
    "        if i % 2 == 0:\n",
    "            dates.append(price)\n",
    "        else:\n",
    "            assert price[0] == \"£\"\n",
    "            prices.append(int(price[1:].replace(\",\", \"\")))\n",
    "    \n",
    "    return dates, prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There's 13 of results.\n",
      "scraping page 0\n",
      "scraping page 1\n",
      "scraping page 2\n",
      "scraping page 3\n",
      "scraping page 4\n",
      "scraping page 5\n",
      "scraping page 6\n",
      "scraping page 7\n",
      "scraping page 8\n",
      "scraping page 9\n",
      "scraping page 10\n",
      "scraping page 11\n",
      "scraping page 12\n"
     ]
    }
   ],
   "source": [
    "properties_info_pages = get_sold_property_info_postcode(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties_info_pages.reset_index(drop=True).to_csv(\"../data/02-explore_right_move/w3_9jj_0.5_mile_flat_sold.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
