import re
import time
from pathlib import Path

import pandas as pd
import requests
from bs4 import BeautifulSoup
from tap import Tap


class Args(Tap):
    url: str  # Right move URL e.g. https://www.rightmove.co.uk/house-prices/w14-0db.html?searchLocation=W140DB&useLocationIdentifier=false&locationIdentifier=&radius=0.5&propertyType=FLAT&soldIn=10&tenure=LEASEHOLD&pageNumber=.
    output_path: Path  # Path to save outputs.


def scrape_sold_houses_right_move(url: str) -> pd.DataFrame:
    """
    Scrape sold houses info from Right Move.

    For the URL:
        1. Go to https://www.rightmove.co.uk/house-prices.html.
        2. Enter your specified search including filters e.g. within 1 mile.
        3. Go to page 2 so the URL includes the "Number=" suffix.
        4. Copy the URL up to "Number=" deleting the 2 at the end.

    Args:
        url (str): URL generated by the above instructions.

    Returns:
        pd.DataFrame: Sold houses info.
    """
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }

    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.text, "html.parser")

    # Get the total number of pages of results.
    n_pages = soup.find_all("div", class_="dsrm_dropdown_section")[0]
    n_pages = n_pages.find_all("span")[1]
    n_pages = int(n_pages.text.replace("of ", ""))

    time.sleep(2)

    properties_info_pages = []

    for i in range(n_pages):
        print(f"Scraping page {i}...")
        response = requests.get(url + str(i + 1), headers=headers)
        soup = BeautifulSoup(response.text, "html.parser")
        properties_info_pages.append(get_sold_property_info(soup))

        # Avoid being blocked by Right move.
        time.sleep(3)

    properties_info_pages = pd.concat(properties_info_pages)

    return properties_info_pages


def get_sold_property_info(soup: BeautifulSoup) -> pd.DataFrame:
    property_cards = soup.find_all("a", attrs={"data-testid": "propertyCard"})

    properties_info = []

    for property_card in property_cards:
        address = property_card.find("h3").text

        property_type = property_card.find_all(
            "div", attrs={"aria-label": re.compile(r"property type:", re.IGNORECASE)}
        )
        if property_type:
            property_type = property_type[0].text.replace("Property Type: ", "")
        else:
            property_type = ""

        bedrooms = property_card.find_all(
            "div", attrs={"aria-label": re.compile(r"bedrooms", re.IGNORECASE)}
        )
        if bedrooms:
            bedrooms = int(bedrooms[0].text.replace("Bedrooms: ", ""))
        else:
            bedrooms = pd.NA

        dates, prices = extract_dates_prices(property_card)

        property_info = pd.DataFrame(
            {
                "property_type": property_type,
                "address": address,
                "date": dates,
                "price": prices,
                "bedrooms": bedrooms,
            }
        )

        properties_info.append(property_info)

    properties_info = pd.concat(properties_info).reset_index(drop=True)

    return properties_info


def extract_dates_prices(property_card):
    prices_dates = property_card.find_all("td")[2:]

    dates = []
    prices = []

    for i, p in enumerate(prices_dates):
        # Reached the end of available dates.
        if p.text == "":
            break

        if i % 2 == 0:
            dates.append(p.text)
        else:
            if p.text[0] == "Â£":
                prices.append(int(p.text[1:].replace(",", "")))
            else:
                print(p.text)
                prices.append("")

    return dates, prices


if __name__ == "__main__":
    args = Args().parse_args()
    sold_house_info = scrape_sold_houses_right_move(args.url)
    sold_house_info.to_csv(args.output_path, index=False)
